{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "import numpy as np\n",
    "from pyemd import emd_with_flow\n",
    "import nltk\n",
    "import nltk.tag.stanford as st\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "import pandas as pd\n",
    "from time import time\n",
    "start_nb = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_signature = np.array([0.0, 1.0]) #a 1 dimensional array (cud be a word vector)\n",
    "second_signature = np.array([5.0, 3.0])\n",
    "distance_matrix = np.array([[0.0, 0.5], [0.5, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.5]\n",
      " [ 0.5  0. ]]\n"
     ]
    }
   ],
   "source": [
    "first_signature\n",
    "second_signature\n",
    "print(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd(first_signature, second_signature, distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.5, [[0.0, 0.0], [0.0, 1.0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd_with_flow(first_signature, second_signature, distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Obama is the President of US. Florida is a nice place. It is good. He lives in Florida. He is visiting India on Monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obama is the President of US. Florida is a nice place. It is good. He lives in Florida. He is visiting India on Monday'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "que_text = \"Who is the President of US.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence_tokenization\n",
    "token_sen = nltk.sent_tokenize(que_text)\n",
    "len(token_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_word(token_sen):\n",
    "    word = []\n",
    "    for i in range(len(token_sen)):\n",
    "        word.append(nltk.word_tokenize(token_sen[i]))\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Who', 'is', 'the', 'President', 'of', 'US', '.']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_word(token_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = st.StanfordNERTagger('/home/aleena/Downloads/stanford-ner-2016-10-31/classifiers/english.muc.7class.distsim.crf.ser.gz', '/home/aleena/Downloads/stanford-ner-2016-10-31/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = \"Obama is the President of US. He is visiting on Tuesday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Obama', 'PERSON'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('President', 'O'),\n",
       " ('of', 'O'),\n",
       " ('US.', 'LOCATION'),\n",
       " ('Florida', 'LOCATION'),\n",
       " ('is', 'O'),\n",
       " ('a', 'O'),\n",
       " ('nice', 'O'),\n",
       " ('place.', 'O'),\n",
       " ('It', 'O'),\n",
       " ('is', 'O'),\n",
       " ('good.', 'O'),\n",
       " ('He', 'O'),\n",
       " ('lives', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Florida.', 'LOCATION'),\n",
       " ('He', 'O'),\n",
       " ('is', 'O'),\n",
       " ('visiting', 'O'),\n",
       " ('India', 'LOCATION'),\n",
       " ('on', 'O'),\n",
       " ('Monday', 'DATE')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_tagged_sent = tagger.tag(text.split())\n",
    "ne_tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ne_tagged_que = tagger.tag(que_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Who', 'O'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('President', 'O'),\n",
       " ('of', 'O'),\n",
       " ('US.', 'LOCATION')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_tagged_que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S Who/WP is/VBZ the/DT President/NNP of/IN (LOCATION US./NNP))\n"
     ]
    }
   ],
   "source": [
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "\n",
    "    return bio_tagged_sent\n",
    "\n",
    "\n",
    "def stanfordNE2tree(ne_tagged_sent):\n",
    "    bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree\n",
    "\n",
    "# ne_tagged_sent = [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), \n",
    "# ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), \n",
    "# ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), \n",
    "# ('in', 'O'), ('Newyork', 'LOCATION')]\n",
    "\n",
    "ne_tree = stanfordNE2tree(ne_tagged_que)\n",
    "\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('US.', 'LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "ne_in_sent = []\n",
    "for subtree in ne_tree:\n",
    "    if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "        ne_label = subtree.label()\n",
    "        ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        ne_in_sent.append((ne_string, ne_label))\n",
    "print(ne_in_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "que_text = \"How is the name the President of US.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence_tokenization\n",
    "token_sen = nltk.sent_tokenize(que_text)\n",
    "len(token_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_word(token_sen):\n",
    "    word = []\n",
    "    for i in range(len(token_sen)):\n",
    "        word.append(nltk.word_tokenize(token_sen[i]))\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How is the name the President of US.']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##list wh determiners and verbs\n",
    "\n",
    "wh = ['WDT', 'WP$', 'WP', 'WRB']\n",
    "verb = ['VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat\n"
     ]
    }
   ],
   "source": [
    "# my_string = \"the cat and this dog are in the garden\"    \n",
    "# splitted = my_string.split()\n",
    "\n",
    "# first = splitted[0] + \" \" + splitted[1]\n",
    "# second = splitted[1]\n",
    "# print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('name', 'NN'), ('the', 'DT'), ('President', 'NNP'), ('of', 'IN'), ('US.', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "pos_tag_word = []\n",
    " \n",
    "for sen in token_sen: \n",
    "    pos_tag_word.append(nltk.pos_tag(sen.split()))\n",
    "    a = nltk.pos_tag(sen.split())\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('When', 'is'): 1, ('is', 'the'): 1, ('President', 'of'): 1, ('of', 'US'): 1, ('coming', '?'): 1, ('the', 'President'): 1, ('US', 'coming'): 1})\n",
      "('When', 'is')\n"
     ]
    }
   ],
   "source": [
    "#extracting bigrams from the question in the dataset\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"When is the President of US coming? \"\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)\n",
    "\n",
    "print(Counter(bigrams)) #counter for n grams\n",
    "bigram_list = list(ngrams(token, 2))\n",
    "print(bigram_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', ['How', 'WRB'])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.chunk.ne_chunk(('How', 'WRB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#put the last element in the forefront\n",
    "array = [1,2,3,4]\n",
    "array[-1]\n",
    "array[:-1] \n",
    "print([array[-1]] + array[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Obama', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('President', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('US.', 'NNP')]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_word[0] #pos_tag for 0th sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is the President of US  Florida is a nice place  It is good  He lives in Florida  Trump is the current president  He owns Trump tower\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      text )  # The text to search\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(letters_only) #converting it to lowwercase is out of question if done so it gives the incorrect NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ne_tag_letter = tagger.tag(letters_only.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Obama',\n",
       " 'is',\n",
       " 'the',\n",
       " 'President',\n",
       " 'of',\n",
       " 'US',\n",
       " 'Florida',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'place',\n",
       " 'It',\n",
       " 'is',\n",
       " 'good',\n",
       " 'He',\n",
       " 'lives',\n",
       " 'in',\n",
       " 'Florida',\n",
       " 'Trump',\n",
       " 'is',\n",
       " 'the',\n",
       " 'current',\n",
       " 'president',\n",
       " 'He',\n",
       " 'owns',\n",
       " 'Trump',\n",
       " 'tower']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_tag_letter\n",
    "words = letters_only.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "#stopword removal\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Obama', 'President', 'US', 'Florida', 'nice', 'place', 'It', 'good', 'He', 'lives', 'Florida', 'Trump', 'current', 'president', 'He', 'owns', 'Trump', 'tower']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compressed function comprising of all the functions\n",
    "def text_to_refined_text(text):\n",
    "          \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    words = letters_only.split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "   \n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When President US coming'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_refined_text(text) #removed punctuations and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a text to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    new_text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    words = new_text.split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    " \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Obama',\n",
       " 'is',\n",
       " 'the',\n",
       " 'President',\n",
       " 'of',\n",
       " 'US',\n",
       " 'Florida',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'place',\n",
       " 'It',\n",
       " 'is',\n",
       " 'good',\n",
       " 'He',\n",
       " 'lives',\n",
       " 'in',\n",
       " 'Florida',\n",
       " 'Trump',\n",
       " 'is',\n",
       " 'the',\n",
       " 'current',\n",
       " 'president',\n",
       " 'He',\n",
       " 'owns',\n",
       " 'Trump',\n",
       " 'tower']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_wordlist(text, remove_stopwords=False) #optionally remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#punckt tokenizer --> as word2vec takes only a list of sentences each one as a list of lists\n",
    "#using pretrained punckt tokenizer\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_sentences(text, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    tokenized_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(tokenized_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(tokenized_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the list of sentences:  1\n"
     ]
    }
   ],
   "source": [
    "#list of list containing word tokenizer for each sentence in the text\n",
    "print(\"length of the list of sentences: \", len(text_to_sentences(text, tokenizer, remove_stopwords= False)))\n",
    "list_of_sentences = text_to_sentences(text, tokenizer, remove_stopwords= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Obama', 'is', 'the', 'President', 'of', 'US'],\n",
       " ['Florida', 'is', 'a', 'nice', 'place'],\n",
       " ['It', 'is', 'good'],\n",
       " ['He', 'lives', 'in', 'Florida'],\n",
       " ['Trump', 'is', 'the', 'current', 'president'],\n",
       " ['He', 'owns', 'Trump', 'tower']]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word vectorization\n",
    "from gensim.models import Word2Vec\n",
    "#odel = word2vec.Word2Vec(list_of_sentences, min_count = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################sklearn#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##tfidf vectorizer\n",
    "text1 = \"John is a programmer.\"\n",
    "text2 = \"John likes coding \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['John', 'is', 'a', 'programmer']]\n",
      "[['John', 'likes', 'coding']]\n"
     ]
    }
   ],
   "source": [
    "a = text_to_sentences(text1, tokenizer, remove_stopwords= False)\n",
    "b = text_to_sentences(text2, tokenizer, remove_stopwords= False)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a corpus with 2 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John is a programmer.', 'John likes coding ']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus = [text1, text2]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John is a programmer.'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words= \"english\") \n",
    "vectors = vectorizer.fit_transform(corpus) \n",
    "vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.579738671538\n",
      "  (0, 3)\t0.814802474667\n",
      "  (1, 1)\t0.449436416524\n",
      "  (1, 2)\t0.631667201738\n",
      "  (1, 0)\t0.631667201738\n"
     ]
    }
   ],
   "source": [
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coding', 'john', 'likes', 'programmer']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.26],\n",
       "       [ 0.26,  1.  ]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((vectors * vectors.T).A, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyse to see semantic similariity\n",
    "#training it for google news about 300 million data to be trained using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.KeyedVectors.load_word2vec_format(\"/home/aleena/Downloads/GoogleNews-vectors-negative300.3.bin\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = model.vocab.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsinvocab = len(vocab)\n",
    "wordsperfile = int(100E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsperfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kitten', 0.7634989619255066)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['puppy', 'cat'], negative=['dog'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('President', 0.8006276488304138),\n",
       " ('chairman', 0.6708745360374451),\n",
       " ('vice_president', 0.6700224876403809),\n",
       " ('chief_executive', 0.6691275238990784),\n",
       " ('CEO', 0.6590125560760498),\n",
       " ('pesident', 0.6265208125114441),\n",
       " ('Vice_President', 0.6216661930084229),\n",
       " ('executive', 0.6182476282119751),\n",
       " ('prez', 0.5761911273002625),\n",
       " ('Presdient', 0.5718376040458679)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"John\", \"John\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('media', 0.6013203859329224),\n",
       " ('reporters', 0.5590886473655701),\n",
       " ('hastily_convened_press', 0.5230372548103333),\n",
       " ('breifing', 0.5228123664855957),\n",
       " ('news', 0.5195180177688599),\n",
       " ('briefing', 0.5145483016967773),\n",
       " ('conference', 0.5127618312835693),\n",
       " ('journalists', 0.5055474042892456),\n",
       " ('presser', 0.5044520497322083),\n",
       " ('secretary_Natalia_Timakova', 0.4820271134376526)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"press\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coding', 'john', 'likes', 'programmer']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = len(text_vocab)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a matrix with shape length of vocab\n",
    "dis_matrix = np.zeros((5,5), dtype = float)\n",
    "dis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (range(len(text_vocab))):\n",
    "    for j in range(len(text_vocab)):\n",
    "        dis_matrix[i][j] = np.round(model.similarity(text_vocab[i], text_vocab[j]), 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.11, -0.02,  0.4 ,  0.  ],\n",
       "       [ 0.11,  1.  ,  0.05,  0.1 ,  0.  ],\n",
       "       [-0.02,  0.05,  1.  ,  0.09,  0.  ],\n",
       "       [ 0.4 ,  0.1 ,  0.09,  1.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40498323473938463"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"coding\",\"programmer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Obamas and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Environment/finite_env/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: addresses, chicago, illinois, media, obama, president, press, speaks\n",
      "['addresses', 'chicago', 'illinois', 'media', 'obama', 'president', 'press', 'speaks']\n"
     ]
    }
   ],
   "source": [
    "d1 = \"Obama speaks to the media in Illinois\"\n",
    "d2 = \"The President addresses the press in Chicago\"\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))\n",
    "\n",
    "text_vocab = vect.get_feature_names()\n",
    "print(text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 1] [1 1 0 0 0 1 1 0]\n",
      "cosine(doc_1, doc_2) = 1.00\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "v_1, v_2 = vect.transform([d1, d2])\n",
    "v_1 = v_1.toarray().ravel()\n",
    "v_2 = v_2.toarray().ravel()\n",
    "print(v_1, v_2)\n",
    "print(\"cosine(doc_1, doc_2) = {:.2f}\".format(cosine(v_1, v_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_len = len(vect.get_feature_names())\n",
    "vec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_matrix = np.zeros((vec_len, vec_len), dtype = float)\n",
    "dis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in (range(len(text_vocab))):\n",
    "    for j in range(len(text_vocab)):\n",
    "        dis_matrix[i][j] = np.round(model.similarity(text_vocab[i], text_vocab[j]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.06,  0.03,  0.06,  0.04,  0.05,  0.1 ,  0.33],\n",
       "       [ 0.06,  1.  ,  0.63,  0.05,  0.51, -0.01,  0.05,  0.09],\n",
       "       [ 0.03,  0.63,  1.  ,  0.02,  0.46, -0.02,  0.03,  0.07],\n",
       "       [ 0.06,  0.05,  0.02,  1.  ,  0.1 ,  0.1 ,  0.6 ,  0.16],\n",
       "       [ 0.04,  0.51,  0.46,  0.1 ,  1.  ,  0.1 ,  0.1 ,  0.04],\n",
       "       [ 0.05, -0.01, -0.02,  0.1 ,  0.1 ,  1.  ,  0.13,  0.14],\n",
       "       [ 0.1 ,  0.05,  0.03,  0.6 ,  0.1 ,  0.13,  1.  ,  0.28],\n",
       "       [ 0.33,  0.09,  0.07,  0.16,  0.04,  0.14,  0.28,  1.  ]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_1 = v_1.astype(\"float64\")\n",
    "v_2 = v_2.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(doc_1, doc_2) = 0.23\n"
     ]
    }
   ],
   "source": [
    "# v_1_1 = v_1/v_1.sum()\n",
    "# v_2_2 = v_2/v_2.sum()\n",
    " # just for comparison purposes\n",
    "print(\"d(doc_1, doc_2) = {:.2f}\".format(emd(v_1, v_2, dis_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Movers Distance (WMD/ EMD/ WMDsimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/data/Environment/finite_env/lib/python35.zip',\n",
       " '/data/Environment/finite_env/lib/python3.5',\n",
       " '/data/Environment/finite_env/lib/python3.5/plat-x86_64-linux-gnu',\n",
       " '/data/Environment/finite_env/lib/python3.5/lib-dynload',\n",
       " '/usr/lib/python3.5',\n",
       " '/usr/lib/python3.5/plat-x86_64-linux-gnu',\n",
       " '/data/Environment/finite_env/lib/python3.5/site-packages',\n",
       " '/data/Environment/finite_env/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/home/aleena/.ipython']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching word embeddings in memmapped format...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"/data/emb.dat\"):\n",
    "    print(\"Caching word embeddings in memmapped format...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"data/emb.dat\"):\n",
    "#     print(\"Caching word embeddings in memmapped format...\")\n",
    "#     from gensim.models.word2vec import Word2Vec\n",
    "    \n",
    "#     fp = np.memmap(\"/data/emb.dat\", dtype=np.double, mode='w+', shape=model.syn0norm.shape)\n",
    "#     fp[:] = model.syn0norm[:]\n",
    "#     with open(\"/data/emb.vocab\", \"w\") as f:\n",
    "#         for _, w in sorted((voc.index, word) for word, voc in model.vocab.items()):\n",
    "#             print(w, file=f)\n",
    "#     del fp, model\n",
    "\n",
    "# W = np.memmap(\"/data/emb.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "# with open(\"/data/emb.vocab\") as f:\n",
    "#     vocab_list = map(str.strip, f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: addresses, chicago, illinois, media, obama, president, press, speaks\n"
     ]
    }
   ],
   "source": [
    "d_obama = \"Obama speaks to the media in Illinois\"\n",
    "d_president = \"The President addresses the press in Chicago\"\n",
    "d_orange = \"Michelle sells oranges by the roadside\"\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "v_1, v_2 = vect.transform([d1, d2])\n",
    "v_1 = v_1.toarray().ravel()\n",
    "v_2 = v_2.toarray().ravel()\n",
    "print(v_1, v_2)\n",
    "print(\"cosine(doc_1, doc_2) = {:.2f}\".format(cosine(v_1, v_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "W_ = W[[vocab_dict[w] for w in vect.get_feature_names()]]\n",
    "D_ = euclidean_distances(W_)\n",
    "print(\"d(addresses, speaks) = {:.2f}\".format(D_[0, 7]))\n",
    "print(\"d(addresses, chicago) = {:.2f}\".format(D_[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyemd needs double precision input\n",
    "v_1 = v_1.astype(np.double)\n",
    "v_2 = v_2.astype(np.double)\n",
    "v_1 /= v_1.sum()\n",
    "v_2 /= v_2.sum()\n",
    "D_ = D_.astype(np.double)\n",
    "D_ /= D_.max()  # just for comparison purposes\n",
    "print(\"d(doc_1, doc_2) = {:.2f}\".format(emd(v_1, v_2, D_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmdistance(document1, document2):\n",
    "    # Some sentences to test.\n",
    "    d_1 = document1.lower().split()\n",
    "    d_2 = document2.lower().split()\n",
    "    # Remove their stopwords.\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    d_1_1 = [w for w in d_1 if w not in stopwords]\n",
    "    d_2_2 = [w for w in d_2 if w not in stopwords]\n",
    "    # Compute WMD.\n",
    "    distance = model.wmdistance(d_1_1, d_2_2) #model already made on a pretrained word embedding\n",
    "    return distance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0621029695429802"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmdistance(d_obama, d_president)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **WMD** uses euclidean distance to find the distance between 2 wordvectors\n",
    "- The euclidean distance between two vectors might be large because their lengths differs. We can control it by        normalizing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21\n"
     ]
    }
   ],
   "source": [
    "# Normalizing word2vec vectors.\n",
    "start = time()\n",
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "distance = wmdistance(d_obama, d_orange) # Compute WMD as normal.\n",
    "print(np.round(distance, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import WmdSimilarity \n",
    "stops = set(stopwords.words(\"english\")) #stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = doc.lower()  # Lower the text.\n",
    "    doc = word_tokenize(doc)  # Split into words.\n",
    "    doc = [w for w in doc if not w in stops]  # Remove stopwords.\n",
    "    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d_obama, d_orange, d_president]\n",
    "instance = WmdSimilarity(corpus, model, num_best=3)\n",
    "\n",
    "start = time()\n",
    "trial_sen = 'Obama is the leader of US.'\n",
    "query = preprocess(trial_sen)\n",
    "sims = instance[query]  # A query is a lookup in the similarity class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Obama is the leader of US.\n",
      "\n",
      "sim = 0.4417\n",
      "Obama speaks to the media in Illinois\n",
      "\n",
      "sim = 0.4383\n",
      "The President addresses the press in Chicago\n",
      "\n",
      "sim = 0.4370\n",
      "Michelle sells oranges by the roadside\n"
     ]
    }
   ],
   "source": [
    "#Given a query sentence..finding the wmdsimilarity of the query with the rest of the sentences in the corpus\n",
    "\n",
    "print('Query:')\n",
    "print(trial_sen)\n",
    "num_best = 3\n",
    "for i in range(num_best):\n",
    "    print()\n",
    "    print('sim = %.4f' % sims[i][1])   \n",
    "    print(corpus[sims[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "##spacy for NER\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Aleena\n",
      "GPE India\n",
      "DATE 2017\n",
      "LANGUAGE English\n",
      "GPE India\n",
      "DATE Monday\n",
      "DATE 7:30pm later this week\n",
      "ORG IIT Bombay\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Ms.Aleena is the President of India on 2017 and she speaks English in India on Monday at 7:30pm later this week in IIT Bombay driving.') \n",
    "doc\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE India\n",
      "PERSON Pranab Mukherjee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Who is the President of India?\")\n",
    "doc1 = nlp(\"Pranab Mukherjee\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "for ent in doc1.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A B C D\n",
      "A 0 2 1 1\n",
      "B 2 0 2 1\n",
      "C 1 2 0 1\n",
      "D 1 1 1 0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "document = [['A', 'B'], ['C', 'B'], ['A', 'B', 'C', 'D']]\n",
    "names = ['A', 'B', 'C', 'D']\n",
    "\n",
    "occurrences = OrderedDict((name, OrderedDict((name, 0) for name in names)) for name in names)\n",
    "\n",
    "# Find the co-occurrences:\n",
    "for l in document:\n",
    "    for i in range(len(l)):\n",
    "        for item in l[:i] + l[i + 1:]:\n",
    "            occurrences[l[i]][item] += 1\n",
    "  #Print the matrix:\n",
    "print(' ', ' '.join(occurrences.keys()))\n",
    "for name, values in occurrences.items():\n",
    "    print(name, ' '.join(str(i) for i in values.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'four']\n",
      "{'four', 'was', 'three', 'The', 'animal'}\n",
      "{'There', 'animals', 'where', 'place', 'was'}\n",
      "{'was'}\n",
      "abcd\n"
     ]
    }
   ],
   "source": [
    "l1 = ['The','animal','was','three', 'four']\n",
    "l2 = ['There','was','place','where','animals']\n",
    "l3 = ['The', 'animal', 'was', 'aleena', 'anjana', 'singam']\n",
    "print(l1[3:])\n",
    "print(set(l1))\n",
    "print(set(l2))\n",
    "print(set(l1).intersection(set(l2)))\n",
    "# for element in l1:\n",
    "#     for element in l2\n",
    "#         if l1[0] == l2[0] & l1[1] == l2[1]\n",
    "\n",
    "if l1[0] == l3[0] and l1[1] == l3[1] and l1[2] == l3[2]:\n",
    "    print(\"abcd\")\n",
    "else:\n",
    "    print(\"aleena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_words(l1, l2):\n",
    "    if l1[0] == l2[0] and l1[1] == l2[1] and l1[2] == l2[2]:\n",
    "        print(l2[3:]) #this prints the elements after the 2nd index in the list2\n",
    "        print(l2[:3]) #prints the element uptil the 2nd index in this context the same elements\n",
    "        print(set(l1).intersection(set(l2))) #prints the same above the common elements..set(removes the duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aleena', 'anjana', 'singam']\n",
      "['The', 'animal', 'was']\n",
      "{'animal', 'The', 'was'}\n"
     ]
    }
   ],
   "source": [
    "common_words(l1, l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 2]\n",
      " [1 0 0 0 0]\n",
      " [0 0 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "docs = ['this this this book',\n",
    "        'this cat good',\n",
    "        'cat good animal']\n",
    "names = ['this', 'book', 'cat', 'good', 'animal']\n",
    "count_model = CountVectorizer(ngram_range=(2,2)) # default unigram model\n",
    "X = count_model.fit_transform(docs)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(Xc.todense()) # print out matrix in dense forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_train = pd.read_json(\"/data/train-v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson\n",
    "sq_train_1 = simplejson.load(open(\"/data/train-v1.1.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method items of dict object at 0x7f4021d31208>\n"
     ]
    }
   ],
   "source": [
    "print(sq_train_1.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'answers': [{'answer...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'answers': [{'answer...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'answers': [{'answer...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'answers': [{'answer...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'answers': [{'answer...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  version\n",
       "0  {'paragraphs': [{'qas': [{'answers': [{'answer...      1.1\n",
       "1  {'paragraphs': [{'qas': [{'answers': [{'answer...      1.1\n",
       "2  {'paragraphs': [{'qas': [{'answers': [{'answer...      1.1\n",
       "3  {'paragraphs': [{'qas': [{'answers': [{'answer...      1.1\n",
       "4  {'paragraphs': [{'qas': [{'answers': [{'answer...      1.1"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = sq_train['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}],\n",
       " 'id': '5733be284776f41900661182',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'}"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]['paragraphs'][0]['qas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
